\section{Related and Future Work}

\paragraph{Genericity of value generation}

The idea of generating random sequences of operations instead of
random internal values is not novel; for example, \qcheck was used as
is to test imperative
programs~\cite{DBLP:journals/sigplan/ClaessenH02}, by generating
random values of an AST of operations, paired to a monadic interpreter
of those syntactic descriptions. However, those examples in the
literature only involve operations for a single return type,
corresponding to the return type of the AST evaluation function. To
integrate operations of distinct return types in the same interface
description, one needs GADTs or some other form of type-level
reasoning.

When multiple value types are involved, we found it helpful to think
of well-typed value generation as term/proof search. Our well-typed
rule to generate random values at type $\tau$ from a function at type
$\sigma \to \tau$ and random values at type $\sigma$ could be
expressed, in term of \qcheck \code{Arbitrary} instances, as
a deduction rule of the form \code{instance Arbitrary b, Arbitrary
  (a -> b) => Arbitrary b where ...}. However, Haskell's type-class
mechanism would not allow this instance deduction rule, as it does not
respect its restrictions for principled, coherent instance
elaboration. Type classes are a helpful and convenient host-language
mechanism, but they are designed for code inference rather than
arbitrary proof search. Our library-level implementation of well-typed
proof search using GADTs gives us more freedom, and is the central
idea of \arti.

It is of course possible to see chaining of function/method calls as
a metaprogramming problem, and generate a reified description of those
calls, to interpret through an external script or reflection/JIT
capability, as done in the testing tool
Randoop~\cite{DBLP:conf/oopsla/PachecoE07}. Doing the generation as
a richly-typed host language library gives us stronger type safety
guarantees: even if our value generator is buggy, it will never
compose operations in a type-incorrect way.

\paragraph{Testing of higher-order or polymorphic interfaces}
\label{sec:higher-order}

The type description language we use captures a first-order subset of
the simply-typed lambda-calculus. A natural question is whether it
would be possible to support random function generation -- embed
negative types into positives. A simple way to generate a function of
type \code{a -> b} is to just generate a \code{b} at each call;
\qcheck additionally uses the \code{a} argument to produce additional
entropy. This is not completely satisfying as it does not use the
argument at type \code{a} (which may not be otherwise reachable from
the interface) to produce new values of type \code{b}. To have full
test coverage for higher-order functional, one should locally add the
argument to the set of known elements at type \code{a}, and
re-generate values at type \code{b} in that extended environment.

It would also be interesting to support representation of polymorphic
operations; we currently only describe monomorphic
instantiations. Bernardy, Jansson and
Claessen~\cite{DBLP:conf/esop/BernardyJC10} have proposed
a parametricity-based technique to derive specific monomorphic
instances for type arguments, which also reduces the search space of
values to be tested. Supporting this technique would be a great asset
of a testing library, but it is definitely not obvious how their
pen-and-paper derivation could be automatized -- and even less so
whether this can be done internally, in the host language of the
implementations being tested.

\paragraph{Bottom-up or top-down generation}

We have presented the \code{ArtiCheck} implementation as a bottom-up
process: from a set of already-discovered values at the types of
interest, we use the constructors of the interface to produce new
values. In contrast, most random checking tools present generation in
a top-down fashion: pick the head constructor of the data value, then
generate its subcomponents recursively. One notable exception is
SmallCheck~\cite{DBLP:conf/haskell/RuncimanNL08}, which performs
exhaustive testing for values of bounded depth.

The distinction is however blurred by several factors. Our
demand-driven computation of fixpoints, implemented by the Fix
library, will result in more elements generated for the types most
useful to the properties we wish to check, giving bottom-up search
a top-down flavor. Relatedly, SmallCheck has a Lazy SmallCheck variant
that uses laziness (demand-driven computation) to avoid fleshing out
value parts that are not inspected by the property being tested.

Furthermore, the genericity of our high-level interface makes
\code{Articheck} amenable to a change in the generation technique; we
could implement direct top-down search without changing the signature
description language, or most parts of the library interface.

\paragraph{Richer property languages}

We discussed in Section~\ref{sec:properties} the difficulty of
isolating an expressive fragment of first-order logic as a property
language that could be given a realizable testing semantics. As it
performs exhaustive search (up to a bound), SmallCheck is able to give
a non-surprising semantics to existential quantification. As we let
user control for each interface datatype whether an exhaustive
collection or random reservoir sampling should be used, we could
support existential on exhaustively collected types only.

In related manner, Berghofer and Nipkow's
implementation of QuickCheck for
Isabelle~\cite{DBLP:conf/sefm/BerghoferN04} stands out by supporting full-fledged
first-order logic for random checking. In the Isabelle proof
assistant, it is common to define computations as inductive
relations/predicates that can be given a (potentially
non-deterministic) functional mode; instead of directly turning
correctness formulas into testing programs, they translate formulas
into inductive datatypes, which are then given a computational
interpretation.

This is remarkable as it not only allows them to support a rich
specification language, but also gives a principled explanation for
the ad-hoc semantics of preconditions in testing frameworks (a failing
precondition does not count as a passed test); instead of seeing
a precondition \code{P[x]} as returning a boolean from
a randomly-generated \code{x}, they choose a mode assignment that
inverts it into a logic program generating the \code{x}s accepted by
the precondition. This gives a logic-based justification to various
heuristics used in other works to generate random values more likely
to pass the precondition, either
domain-specific~\cite{DBLP:conf/tap/ClaessenS08} or
SAT-based~\cite{DBLP:conf/tap/AhnD10}.

\section{Implementing \arti}
\label{sec:representation}

The simplistic design we introduced in \sref{sec:essence} conveys the main ideas
behind \arti, yet fails to address a wide variety of problems. The present section
reviews the issues with the current design and incrementally addresses them.

\subsection{A better algebra of types}
\label{sec:algebra}

The simply-typed lambda calculus that we introduced only contains constants and
functions. While one can theoretically encode sums and products using functions,
it seems reasonable to have a built-in notion of sums and products in our
language.

One of the authors naïvely suggested that the data type be extended with cases
for products and sums, such as:
%
\begin{ocamlcode}
| Prod: ('a,'c) fn * ('b,'c) fn -> ('a * 'b,'c) fn
\end{ocamlcode}
%
It turns out that the branch above does not describe products. If \code{'a} is
\code{int -> int} and \code{'b} is \code{int -> float}, not only do the
\code{'c} parameters fail to match, but the \code{'a * 'b} parameter in the
conclusion represents a pair of functions, rather than a function that returns a
pair! Another snag is that the type of \code{eval} makes no sense in the case of
a product. If the first parameter of type \code{('a, 'b) fn} represents a way to
obtain a \code{'b} from the product type \code{'a}, then what use is the second
parameter of \code{eval}?

In light of these limitations, we take inspiration from the literature on
% TODO \cite
focusing and break the \code{fn} type into two distinct GADTs.
\begin{itemize}
  \item The GADT \code{('a, 'b) negative} (\code{neg} for short) represents a
    \emph{computation} of type \code{'a} that produces a result of type
    \code{'b}.
  \item The GADT \code{'a positive} (\code{pos} for short) represents a
    \emph{value}, that is, the result of a computation.
\end{itemize}
%
% 3a9c140
\begin{ocamlcode}
type (_, _) neg =
| Fun : 'a pos * ('b, 'c) neg -> ('a -> 'b, 'c) neg
| Ret : 'a pos -> ('a, 'a) neg

and _ pos =
| Ty : 'a ty -> 'a pos
| Sum : 'a pos * 'b pos -> ('a, 'b) sum pos
| Prod : 'a pos  * 'b pos -> ('a * 'b) pos
| Bij : 'a pos * ('a, 'b) bijection -> 'b pos

and ('a, 'b) sum = L of 'a | R of 'b
\end{ocamlcode}
%
The \code{pos} type represents first-order data types: products, sums and atomic
types, that is, whatever is on the rightmost side of an arrow. We provide an
injection from positive to negative types via the \code{Ret} constructor: a
value of type \code{'a} is also a constant computation.

We do \emph{not} provide an injection from negative types to positive types:
this would allow nested arrows, that is, higher-order types.  One can take the
example of the \code{map} function, which has type \code{('a -> 'b) -> 'a list
-> 'b list}: we explicitly disallow
representing the \code{'a -> 'b} part as a
\code{Fun} constructor, as it would require us to synthesize instances of a
function type (see \sref{sec:higher-order} for a discussion).
Rather, we ask the user to represent \code{'a -> 'b} as a \code{Ty} constructor;
in other words, we ask the user to supply their own test functions as if they
were a built-in type.

Our GADT does not accurately model tagged, n-ary sums of OCaml, nor records with
named fields. We thus add a last \code{Bij} case; it allows the user to
provide a two-way mapping between a built-in type (say, \code{'a option}) and
its \arti representation (\code{() + 'a}). That way, \arti can work with regular
OCaml data types by converting them back-and-forth.

This change of representation incurs some changes on our evaluation functions
as well. The \code{eval} function is split into several parts, which we detail
right below.
%
\begin{ocamlcode}
let rec apply: type a b. (a, b) neg -> a -> b list =
  fun ty v -> match ty with
  | Fun (p, n) ->
      produce p |> concat_map (fun a -> apply n (v a))
  ...
and produce: type a. a pos -> a list =
  fun ty -> match ty with
  | Ty ty -> ty.enum
  | Prod (pa, pb) ->
      cartesian_product (produce pa) (produce pb)
  ...
let rec destruct: type a. a pos -> a -> unit =
  function
  | Ty ty -> (fun v ->
      remember v ty)
  | Prod (ta, tb) -> (fun (a, b) ->
      destruct ta a;
      destruct tb b)
  ...

(* Putting it all together *)
let _ =
  ...
  let li = apply fd f in
  List.iter destruct li;
  ...
\end{ocamlcode}
%
Let us first turn to the case of \emph{values}. In order to understand what \arti
ought to do, one may ask themselves what the user can do with values. The user
may destruct them: given a pair of type \code{'a * 'b}, the user may keep 
the first element only, thus obtaining a new \code{'a}. The same goes for sums. We
thus provide a \code{destruct} function, which breaks down positives types by
pattern-matching, populating the descriptions of the various types it encounters
as it goes. (The \code{remember} function records all instances we haven't
encountered yet in the type descriptor \code{ty}.)

Keeping this in mind, we must realize that if a function takes an \code{'a}, the
user may use any \code{'a} it can produce to call the function. For instance, in
the case that \code{'a} is a product type \code{'a1 * 'a2}, then \emph{any} pair
of \code{'a1} and \code{'a2} may work.  We introduce a function called
\code{produce}, which reflects the fact the user may choose any possible pair:
the function exhibits the entire set of instances we can build for a given type.

Finally, the \code{apply} function, just like before, takes a \emph{computation}
along with a matching description, and generates a set of \code{b}. However, it
now relies on \code{product} to exhaustively exhibit all possible arguments one
can pass to the function.

We are now able to accurately model a calculus rich enough to test realistic
signatures involving records, option types, and various ways to create
functions.

\subsection{Efficient representation of a set of instances}
\label{sec:efficient}

The (assuredly naïve) scenario above reveals several pain points with the
current design.
\begin{itemize}
  \item We represent our sets using lists. We could use a more efficient data
    structure.
  \item If some function takes, say, a tuple, the code as it stands will
    construct the set of all possible tuples, \code{map} the function over the
    set, then finally call \code{destruct} on each resulting element to collect
    instances.  Naturally, memory explosion ensues. We propose a symbolic
    algebra for \emph{sets of instances} that \emph{mirrors} the structure of
    positive types and avoids the need for holding all possible combinations in
    memory at the same time.
  \item A seemingly trivial optimization sends us off the rails by generating an
    insane number of instances. We explain how to optimize further the code
    while still retaining a well-behaved generation.
  \item Fairness issues arise.
    Take the example of logical formulas. One may try to be smart: starting with
    constants, one may apply \code{mk\_and}, then pass the freshly generated
    instances to \code{mk\_xor}. A consequence is that all the formulas with two
    combinators start with \code{xor}. If we just keep an iterative process and
    do not chain the instance generation process, formulas containing three
    combinators are only reached after we've exhausted all possible instances
    with two or less combinators. This breadth-first search of the instance
    space is sub-optimal. Can we do better?
\end{itemize}
%

\paragraph{Sets of instances}
The first, natural optimization that comes to mind consists in dropping lists in
favor of a more sophisticated data type. We replace lists with a module
\code{PSet} of polymorphic, persistent sets implemented as red-black trees.


\paragraph{Not holding sets in memory}
A big source of inefficiency is the call to the
\code{cartesian\_product} function above (\sref{sec:algebra}). We hold in memory
at the same time all possible products, then pipe them into the function calls
so as to generate an even bigger set of elements. Only when the set of all
elements has been constructed do we actually run \code{destruct}, only to
extract the instances that we have created in the process.

Holding in memory the set of all possible products is too expensive. We adopt
instead a \emph{symbolic representation of sets}, where unions and products are
explicitly represented using constructors. This mirrors our algebra of positive
types.
%
% 29bab9d5d66f066906b5b3d1449fd02cf34aa7dc
\begin{ocamlcode}
type _ set =
  | Set   : 'a PSet.t -> 'a set
  | Bij   : 'a set * ('a, 'b) bijection -> 'b set
  | Union   : 'a set * 'b set -> ('a,'b) sum set
  | Product : 'a set * 'b set -> ('a * 'b) set
\end{ocamlcode}
%
This does not suppress the combinatorial explosion. The instance space is still
exponentially large; what we gained by changing our representation is that we
no longer hold all the ``intermediary'' instances in memory
\emph{simultaneously}. This allows us to write an \code{iter} function that
constructs the various instances on-the-fly.
%
\begin{ocamlcode}
let rec iter: type a. (a -> unit) -> a set -> unit =
fun f s -> match s with
  | Set ps ->
      PSet.iter f ps
  | Union (pa,pb) ->
      iter (fun a -> f (L a)) pa;
      iter (fun b -> f (R b)) pb;
  | Product (pa,pb) ->
      iter (fun a -> iter (fun b -> f (a,b)) pb) pa
  | (* ... *)
\end{ocamlcode}
%

\paragraph{Piping and non-termination}
In order to push the optimization above further, one can choose to perform the
call to \code{remember} directly inside the \code{Ret} case of \code{apply}.
That way, \code{apply} could just fill in the type descriptors using the global,
mutable state and return unit, thus avoiding the need for intermediary lists of
instances. Also, calling \code{remember} directly eliminates the need to store
duplicate items, as the function automatically takes care of dropping an
instance if we are already aware of it.

This seemingly innocuous optimization raised combinatorial explosion issues. We
explain why, in the hope that it serves as an example for future generations
(``kids, don't do mutable state'').

Consider the case of a function that has type \code{t -> t -> t} and a
corresponding type descriptor for \code{t} named \code{ty}. The outer call
to \code{apply} binds the list of instances of \code{t} via \code{let l =
ty.enum}. For each element of \code{l}, a recursive call to \code{apply} takes
place (for the inner \code{t -> t} function), which looks up the current value
of \code{ty.enum}. Since each inner call populates \code{ty.enum} itself, for
each new recursive call of \code{apply}, the value of \code{ty.enum} grows
bigger and bigger. The programs terminates by exhausting its memory space
without even returning from the outer call to \code{apply}.

We solved this by taking a snapshot of our negative types before calling
\code{apply}. No copy is involved: function arguments (positive types) are
represented in memory as persistent, pure symbolic sets. That way, we keep a
copy of the arguments that are to be applied in each \code{Fun} case.

\paragraph{Fairness of our search space}
% TODO jp: we have to redo this part entirely (see my recap in the itemize
% section at the beginning)
Snapshotting enforces a breadth-first search of the instance space. The initial
set of instances is fed through the available functions, and we iterate the
process, until we've obtained a satisfactory number of instances for each one of
the types we wish to test.

The distribution of instances is skewed: there are more instances obtained after
\code{n} calls than there are after \code{n+1} calls. It may thus be the case
that by the time we reach three of four consecutive function calls, we've hit
the maximum limit of instances allowed for the type, since it often is the case
that the number of instances grow exponentially.

We plan to implement a random search of the instance space and tweak our
exploration procedures so that ``interesting'' instances pop up early.

\subsection{Instance generation as a fixed point computation}

The \code{apply}/\code{destruct} combination only demonstrates how to generate
new instances from one specific element of the signature. We need to iterate
this recipe on the whole signature, by feeding the new instances that we
obtain to other functions that can in turn consume them.

This part of the problem naturally presents itself as a fixpoint computation,
defined by a system of equations. Equations between variables (type descriptors)
describe ways of obtaining new instances (by applying functions to other type
descriptors). Of course, to ensure termination, we need to put a bound on the
number of generated instances. When presenting an algorithm as a fixpoint
problem, it is indeed a fairly standard technique to make the lattice space
artificially finite in order to obtain the termination property.

Implementing an efficient fixpoint computation is a \emph{surprisingly
interesting} activity, and we are happy to use an off-the-shelf fixpoint
library, F. Pottier's \code{Fix}~\cite{fix},
to perform the work for us. \code{Fix} can be summarized by the signature
below, obtained from user-defined instantiations of the types \code{variable}
and \code{property}.
%
\begin{ocamlcode}
module Fix = sig
  type valuation = variable -> property
  type rhs = valuation -> property
  type equations = variable -> rhs

  val lfp: equations -> valuation
end
\end{ocamlcode}
%
A system of equations maps a variable to a right-hand side.  Each right-hand
side can be evaluated by providing a valuation so as to obtain a property.
Valuations map variables to properties. Solving a system of equations amounts to
calling the \code{lfp} function which, given a set of equations, returns the
corresponding valuation.

A perhaps tempting way to fit in this setting would be to define variables to be
our \code{'a ty} (type descriptor) and properties to be \code{'a list}s (the
instances we have built so far); the equations derived from any signature would
then describe ways of obtaining new instances by applying any function of the
signature. This doesn't work as is: since there will be multiple values of
\code{'a} (we generate instances of different types simultaneously), type
mismatches are to be expected. One could, after all, use yet another GADT and
hide the \code{'a} type parameter behind an existential variable.
%
\begin{ocamlcode}
  type variable = Atom: 'a ty -> variable
  type property = Props: 'a set -> property
\end{ocamlcode}
%
The problem is that there is no way to statically prove that having an
\code{'a var} named \code{x}, calling \code{valuation x} yields an
\code{'a property} with a matching type parameter. This is precisely
where the mutable state in the \code{'a ty} type comes handy: even though it is
only used as the \emph{input} parameter for the system of equations, we
``cheat'' and use its mutable \code{enum} field to store the output. That way,
the \code{property} type needs not mention the type variable \code{'a} anymore,
thus removing any typing difficulty -- or the need to change the interface of
\code{Fix}.

We still, however, need the \code{property} type to be a rich enough lattice to
let \code{Fix} decide when to stop iterating: it should come with equality- and
maximality-checking functions, used by \code{Fix} to detect that the fixpoint is
reached. The solution is to define \code{property} as the number of instances
generated so far along with the bound we have chosen in advance:
%
\begin{ocamlcode}
  type variable = Atom : 'a ty -> variable
  type property = { required : int; produced : int }
  let equal p1 p2 = p1.produced = p2.produced
  let is_maximal p = p.produced >= p.required
\end{ocamlcode}
%
